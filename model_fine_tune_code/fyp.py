# -*- coding: utf-8 -*-
"""FYP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dVVOUshHkYCDlCZ1kpj-OsIDzuNPNxFf
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import (
    f1_score, roc_auc_score, accuracy_score,
    precision_score, recall_score, classification_report,
    hamming_loss, jaccard_score
)
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    TrainingArguments,
    Trainer
)

# ============================================================================
# SECTION 1: DATA PREPARATION WITH PROPER SPLITS
# ============================================================================

print("="*80)
print("LOADING GOEMOTIONS DATASET")
print("="*80)

# Load GoEmotions dataset
dataset = load_dataset("go_emotions", "simplified")
emotion_labels = dataset["train"].features["labels"].feature.names
NUM_LABELS = len(emotion_labels)

# Create label mappings
id2label = {i: label for i, label in enumerate(emotion_labels)}
label2id = {label: i for i, label in enumerate(emotion_labels)}

print(f"Total emotions: {NUM_LABELS}")
print(f"Emotion labels: {emotion_labels}")
print(f"\nDataset splits:")
print(f"  Train: {len(dataset['train'])} samples")
print(f"  Validation: {len(dataset['validation'])} samples")
print(f"  Test: {len(dataset['test'])} samples")


def convert_labels_to_multihot(batch):
    """Convert multi-label format to binary vector"""
    label_vector = np.zeros(NUM_LABELS, dtype=np.float32)
    for label_id in batch["labels"]:
        label_vector[label_id] = 1.0
    return {"labels": label_vector.tolist()}


# Apply conversion
dataset_encoded = dataset.map(convert_labels_to_multihot)

# ============================================================================
# SECTION 2: BASELINE MODEL (TF-IDF + LOGISTIC REGRESSION)
# ============================================================================

print("\n" + "="*80)
print("TRAINING BASELINE MODEL (TF-IDF + Logistic Regression)")
print("="*80)

# Prepare text data
train_texts = dataset_encoded["train"]["text"]
train_labels = np.array([x for x in dataset_encoded["train"]["labels"]])

test_texts = dataset_encoded["test"]["text"]
test_labels = np.array([x for x in dataset_encoded["test"]["labels"]])

# TF-IDF Vectorization
print("\nVectorizing text with TF-IDF...")
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_train_tfidf = tfidf.fit_transform(train_texts)
X_test_tfidf = tfidf.transform(test_texts)

# Train baseline classifier
print("Training baseline classifier...")
baseline_model = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))
baseline_model.fit(X_train_tfidf, train_labels)

# Evaluate baseline
print("\nEvaluating baseline model...")
baseline_pred = baseline_model.predict(X_test_tfidf)
baseline_pred_proba = baseline_model.predict_proba(X_test_tfidf)

baseline_metrics = {
    "micro_f1": f1_score(test_labels, baseline_pred, average="micro"),
    "macro_f1": f1_score(test_labels, baseline_pred, average="macro"),
    "micro_precision": precision_score(test_labels, baseline_pred, average="micro"),
    "micro_recall": recall_score(test_labels, baseline_pred, average="micro"),
    "hamming_loss": hamming_loss(test_labels, baseline_pred),
    "jaccard": jaccard_score(test_labels, baseline_pred, average="micro")
}

print("\n--- BASELINE MODEL RESULTS ---")
for metric, value in baseline_metrics.items():
    print(f"{metric}: {value:.4f}")

# ============================================================================
# SECTION 3: TRANSFORMER MODEL (DISTILBERT)
# ============================================================================

print("\n" + "="*80)
print("PREPARING DISTILBERT MODEL")
print("="*80)

model_checkpoint = "distilbert-base-uncased"
tokenizer = DistilBertTokenizerFast.from_pretrained(model_checkpoint)
MAX_LENGTH = 512

def tokenize_and_prepare_input(examples):
    """Tokenize text for transformer input"""
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LENGTH
    )

# Tokenize datasets
tokenized_datasets = dataset_encoded.map(
    tokenize_and_prepare_input,
    batched=True,
    remove_columns=['text', 'id']
)

# Set proper data types
from datasets import Value
new_features = tokenized_datasets['train'].features.copy()
new_features['labels'] = [Value('float32')]
tokenized_datasets = tokenized_datasets.cast(new_features)
tokenized_datasets.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# Load model
model = DistilBertForSequenceClassification.from_pretrained(
    model_checkpoint,
    num_labels=NUM_LABELS,
    id2label=id2label,
    label2id=label2id,
    problem_type="multi_label_classification"
)

print(f"\nModel classifier output dimension: {model.classifier.out_features}")
print("Loss function: Binary Cross Entropy with Logits (BCEWithLogitsLoss)")

# ============================================================================
# SECTION 4: TRAINING WITH PROPER METRICS
# ============================================================================

def compute_metrics(eval_pred):
    """
    Comprehensive metrics for multi-label classification
    Reports both micro (overall) and macro (per-class) averages
    """
    logits, labels = eval_pred
    probabilities = 1 / (1 + np.exp(-logits))  # Sigmoid
    predictions = (probabilities > 0.5).astype(int)

    return {
        "micro_f1": f1_score(labels, predictions, average="micro"),
        "macro_f1": f1_score(labels, predictions, average="macro"),
        "micro_precision": precision_score(labels, predictions, average="micro", zero_division=0),
        "micro_recall": recall_score(labels, predictions, average="micro", zero_division=0),
        "hamming_loss": hamming_loss(labels, predictions),
        "roc_auc": roc_auc_score(labels, probabilities, average="micro") if len(np.unique(labels)) > 1 else 0.0,
    }


training_args = TrainingArguments(
    output_dir="./results_goemotions",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    learning_rate=2e-5,
    logging_dir='./logs',
    logging_steps=100,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="micro_f1",
    report_to="none",
    seed=42
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    processing_class=tokenizer,
    compute_metrics=compute_metrics
)

print("\n" + "="*80)
print("STARTING TRAINING")
print("="*80)

# Train the model
trainer.train()

# ============================================================================
# SECTION 5: TEST SET EVALUATION (CRITICAL FOR RESEARCH)
# ============================================================================

print("\n" + "="*80)
print("EVALUATING ON TEST SET")
print("="*80)

# Evaluate on test set
test_results = trainer.evaluate(tokenized_datasets["test"])

print("\n--- DISTILBERT TEST RESULTS ---")
for metric, value in test_results.items():
    if metric.startswith("eval_"):
        print(f"{metric.replace('eval_', '')}: {value:.4f}")

# ============================================================================
# SECTION 6: PER-EMOTION ANALYSIS (IMPORTANT FOR DISCUSSION)
# ============================================================================

print("\n" + "="*80)
print("PER-EMOTION PERFORMANCE ANALYSIS")
print("="*80)

# Get predictions for test set
predictions = trainer.predict(tokenized_datasets["test"])
logits = predictions.predictions
labels = predictions.label_ids

probabilities = 1 / (1 + np.exp(-logits))
pred_labels = (probabilities > 0.5).astype(int)

# Calculate per-emotion F1 scores
per_emotion_f1 = []
for i, emotion in enumerate(emotion_labels):
    f1 = f1_score(labels[:, i], pred_labels[:, i])
    per_emotion_f1.append((emotion, f1))

per_emotion_f1.sort(key=lambda x: x[1], reverse=True)

print("\nTop 10 Best Performing Emotions:")
for emotion, f1 in per_emotion_f1[:10]:
    print(f"  {emotion:15s}: {f1:.4f}")

print("\nBottom 10 Worst Performing Emotions:")
for emotion, f1 in per_emotion_f1[-10:]:
    print(f"  {emotion:15s}: {f1:.4f}")

# ============================================================================
# SECTION 7: COMPARISON TABLE (FOR DISSERTATION)
# ============================================================================

print("\n" + "="*80)
print("MODEL COMPARISON SUMMARY")
print("="*80)

comparison_df = pd.DataFrame({
    "Metric": ["Micro F1", "Macro F1", "Micro Precision", "Micro Recall", "Hamming Loss"],
    "Baseline (TF-IDF + LR)": [
        baseline_metrics["micro_f1"],
        baseline_metrics["macro_f1"],
        baseline_metrics["micro_precision"],
        baseline_metrics["micro_recall"],
        baseline_metrics["hamming_loss"]
    ],
    "DistilBERT": [
        test_results["eval_micro_f1"],
        test_results["eval_macro_f1"],
        test_results["eval_micro_precision"],
        test_results["eval_micro_recall"],
        test_results["eval_hamming_loss"]
    ]
})

print("\n", comparison_df.to_string(index=False))

# Calculate improvement
improvement = ((test_results["eval_micro_f1"] - baseline_metrics["micro_f1"])
               / baseline_metrics["micro_f1"] * 100)
print(f"\nRelative Improvement: {improvement:.2f}%")

# ============================================================================
# SECTION 8: SAVE MODEL AND RESULTS
# ============================================================================

save_path = "/content/drive/MyDrive/models/emotion_model_research"
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

# Save results for dissertation
results_summary = {
    "baseline_metrics": baseline_metrics,
    "transformer_metrics": {k.replace("eval_", ""): v for k, v in test_results.items()},
    "per_emotion_f1": per_emotion_f1,
    "improvement_percentage": improvement
}

import json
with open(f"{save_path}/results_summary.json", "w") as f:
    json.dump(results_summary, f, indent=2)

print(f"\nModel and results saved to {save_path}")

# ============================================================================
# SECTION 9: JOURNAL ENTRY TESTING (RESEARCH VALIDATION)
# ============================================================================

print("\n" + "="*80)
print("TESTING ON JOURNAL-STYLE TEXT")
print("="*80)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# Test on different types of journal entries
journal_samples = [
    "I finished the project today! I'm so proud of myself and excited for what's next, but I'm also relieved the stress is over.",
    "Today was difficult. I feel overwhelmed by everything happening at once and I'm not sure how to handle it all.",
    "Had a quiet day at home. Nothing special happened but I feel content and peaceful.",
    "I'm angry at myself for procrastinating again. This always happens and I don't know why I can't change."
]

print("\nAnalyzing journal entries:\n")

for idx, entry in enumerate(journal_samples, 1):
    print(f"Entry {idx}: \"{entry[:60]}...\"")

    test_input = tokenizer(
        entry,
        truncation=True,
        padding="max_length",
        max_length=MAX_LENGTH,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        outputs = model(**test_input)
        logits = outputs.logits

    probabilities = torch.sigmoid(logits).squeeze().cpu().numpy()

    # Get top 5 emotions
    top_indices = np.argsort(probabilities)[-5:][::-1]

    print("  Top 5 Emotions:")
    for i in top_indices:
        print(f"    {emotion_labels[i]:15s}: {probabilities[i]*100:.2f}%")
    print()

# ============================================================================
# SECTION 8: SAVE MODEL AND RESULTS (OPTIMIZED)
# ============================================================================

save_path = "/content/drive/MyDrive/models/emotion_model_research_optimized"

# Save model with safetensors format (faster, safer)
print("\nSaving model with safetensors format...")
model.save_pretrained(save_path, safe_serialization=True)
tokenizer.save_pretrained(save_path)

# Save results for dissertation
results_summary = {
    "baseline_metrics": baseline_metrics,
    "transformer_metrics": {k.replace("eval_", ""): v for k, v in test_results.items()},
    "per_emotion_f1": per_emotion_f1,
    "improvement_percentage": improvement
}

import json
with open(f"{save_path}/results_summary.json", "w") as f:
    json.dump(results_summary, f, indent=2)

# Add metadata for production use
metadata = {
    "model_name": "DistilBERT Multi-Label Emotion Detection",
    "base_model": "distilbert-base-uncased",
    "dataset": "GoEmotions (simplified)",
    "task": "multi-label-classification",
    "num_labels": NUM_LABELS,
    "emotions": emotion_labels,
    "max_length": MAX_LENGTH,
    "training_epochs": 3,
    "best_micro_f1": test_results["eval_micro_f1"],
    "format": "safetensors",
    "framework": "transformers",
    "pytorch_version": torch.__version__,
    "date_trained": "2024"
}

with open(f"{save_path}/metadata.json", "w") as f:
    json.dump(metadata, f, indent=2)

print(f"\nâœ“ Model saved to {save_path}")
print(f"  Format: Safetensors (optimized for production)")
print(f"  Files saved:")
print(f"    - model.safetensors (model weights)")
print(f"    - config.json (model configuration)")
print(f"    - tokenizer files (vocab.txt, tokenizer_config.json, etc.)")
print(f"    - results_summary.json (training metrics)")
print(f"    - metadata.json (model information)")